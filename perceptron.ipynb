{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perzeptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein paar Hilfsfunktionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(888)\n",
    "\n",
    "# generate fake data that is linearly separable with a margin epsilon given the data\n",
    "def getfake(samples, dimensions, epsilon):\n",
    "    wfake = rng.standard_normal(size=(dimensions))   # fake weight vector for separation\n",
    "    bfake = rng.standard_normal(size=(1))            # fake bias\n",
    "    wfake = wfake / np.linalg.norm(wfake)                 # rescale to unit length\n",
    "\n",
    "    # making some linearly separable data, simply by chosing the labels accordingly\n",
    "    X = np.zeros(shape=(samples, dimensions))\n",
    "    Y = np.zeros(shape=(samples))\n",
    "\n",
    "    i = 0\n",
    "    while (i < samples):\n",
    "        tmp = rng.standard_normal(size=(1,dimensions))\n",
    "        margin = np.dot(tmp, wfake) + bfake\n",
    "        if (np.linalg.norm(tmp) < 3) & (abs(margin) > epsilon):\n",
    "            X[i,:] = tmp[0]\n",
    "            Y[i] = np.sign(margin)\n",
    "            i += 1\n",
    "    return X, Y\n",
    "\n",
    "# plot the data with colors chosen according to the labels\n",
    "def plotdata(X,Y):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, alpha=0.7)\n",
    "\n",
    "# plot contour plots on a [-3,3] x [-3,3] grid\n",
    "def plotscore(w,d):\n",
    "    xgrid = np.arange(-3, 3, 0.02)\n",
    "    ygrid = np.arange(-3, 3, 0.02)\n",
    "    xx, yy = np.meshgrid(xgrid, ygrid)\n",
    "    zz = np.zeros(shape=(xgrid.size, ygrid.size, 2))\n",
    "    zz[:,:,0] = np.array(xx)\n",
    "    zz[:,:,1] = np.array(yy)\n",
    "    vv = np.dot(zz,w) + d\n",
    "    CS = plt.contour(xgrid,ygrid,vv)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "X, Y = getfake(50, 2, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotten wir unsere synthetischen, linear separierbaren Daten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt definieren wir unser Perzeptron:\n",
    "\n",
    "$$\\hat{y}=sgn(w^T x + b)$$\n",
    "\n",
    "$\\hat{y}$ ist positiv oder negativ je nachdem, ob x ober- oder unterhalb der durch $(w, b)$ definierten Gerade ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(w, b, idx, x, y):\n",
    "    y_hat = np.sign(np.dot(w,x) + b)\n",
    "    if idx >= 0: print(f'Datensatz({idx}):   {x}, label {y}')\n",
    "    if (y * y_hat) <= 0:\n",
    "        if idx >= 0: print(f'Anpassung da falsche Klassifikation, sollte {y} sein')\n",
    "        w += y * x\n",
    "        b += y\n",
    "        if idx >= 0: print(f'Neue Weight {w}, Bias  {b}')\n",
    "        return 1\n",
    "    else:\n",
    "        if idx >= 0: print('keine Anpassung, richtig klassifiziert')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ... und schicken es auf die Reise ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.zeros(shape=(2))\n",
    "b = np.zeros(shape=(1))\n",
    "for idx, (x,y) in enumerate(zip(X,Y)):\n",
    "    res = perceptron(w,b,idx, x,y)\n",
    "    if (res == 1):\n",
    "        plotscore(w,b)\n",
    "        plotdata(X,Y)\n",
    "        plt.scatter(x[0], x[1], color='g', s=100)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron Convergence Theorem\n",
    "\n",
    "Das Theorem besagt, dass, wenn alle Samples innerhalb eines Kreises mit Radius R liegen und die Punkte mit einem Abstand von mindestens $\\epsilon$ linear separierbar sind, dann konvergiert das Updateverfahren und benötigt maximal $\\frac{2(R^2+1)}{\\epsilon^2}$ Updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eps = np.arange(0.025, 2., 0.05)\n",
    "Err = np.zeros(shape=(Eps.size))\n",
    "\n",
    "n = 10 # Anzahl Durchläufe\n",
    "\n",
    "for j in range(n):\n",
    "    for (i,epsilon) in enumerate(Eps):\n",
    "        X, Y = getfake(500, 2, epsilon)\n",
    "\n",
    "        for (x,y) in zip(X,Y):\n",
    "            Err[i] += perceptron(w,b,-1, x,y)\n",
    "\n",
    "Err = Err / n\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8,8))\n",
    "ax.plot(Eps, Err, label='Durchschnittliche Anzahl an Updates für Training')\n",
    "ax.plot(Eps, 20/(Eps**2), label='Theoretisch maximale Anzahl an Updates')\n",
    "ax.set_ylim([0, 35])\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('dhbw20': conda)",
   "language": "python",
   "name": "python37664bitdhbw20conda4e9f90a409f24fc4a1b2fdf8774acce3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
