{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge und Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import Gauss\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overfitting\n",
    "\n",
    "Die Einführung von Basisfunktionen in unsere lineare Regression macht das Modell\n",
    "sehr viel flexibler, kann aber auch sehr schnell zu einer Überanpassung (Overfitting) führen.\n",
    "\n",
    "Wenn wir z.B. zu viele Gauß'sche Basisfunktionen wählen, erhalten wir am Ende\n",
    "Ergebnisse, die nicht so gut aussehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "x = 10 * rng.random(50)\n",
    "y = np.sin(x) + 0.1 * rng.standard_normal(50)\n",
    "\n",
    "model = make_pipeline(Gauss.GaussianFeatures(30),\n",
    "                      LinearRegression())\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.5, 1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Da die Daten auf die 30-dimensionale Basis projiziert werden, ist das Modell viel\n",
    "zu flexibel und nimmt Extremwerte an, um sich möglichst genau an die Daten\n",
    "anzupassen. Das Problem dabei ist, dass Vorhersagen für neue Werte dann nichts taugen, denn die Kurve hat\n",
    "sich auf die vorhandenen Werte spezialisiert und lässt sich nicht verallgemeinern.\n",
    "\n",
    "Wir können den Grund dafür erkennen, wenn wir die Koeffizienten der Gauß'schen Basen\n",
    "in Relation zu $x$ darstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def basis_plot(model, title=None):\n",
    "    fig, ax = plt.subplots(2, sharex=True)\n",
    "    model.fit(x[:, np.newaxis], y)\n",
    "    ax[0].scatter(x, y)\n",
    "    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\n",
    "\n",
    "    if title:\n",
    "        ax[0].set_title(title)\n",
    "\n",
    "    ax[1].plot(model.steps[0][1].centers_,\n",
    "               model.steps[1][1].coef_)\n",
    "    ax[1].set(xlabel='basis location',\n",
    "              ylabel='coefficient',\n",
    "              xlim=(0, 10))\n",
    "\n",
    "model = make_pipeline(Gauss.GaussianFeatures(30), LinearRegression())\n",
    "basis_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Das untere Feld dieser Abbildung zeigt die Amplitude der Basisfunktion an jedem\n",
    "Ort. Dies ist ein typisches Overfitting-Verhalten, wenn sich Basisfunktionen\n",
    "überlappen: die Koeffizienten benachbarter Basisfunktionen explodieren und heben\n",
    "sich gegenseitig auf.\n",
    "\n",
    "## Regularisierung\n",
    "\n",
    "Wir wissen, dass ein solches Verhalten problematisch ist, und es wäre schön, wenn\n",
    "wir solche Spitzen im Modell explizit begrenzen könnten, indem wir große Werte der\n",
    "Modellparameter bestrafen. Eine solche Bestrafung ist als **Regularisierung**\n",
    "bekannt und kommt in verschiedenen Formen vor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ridge-Regression ($L_2$ Regularisierung)\n",
    "\n",
    "Die vielleicht häufigste Form der Regularisierung ist bekannt als Ridge-Regression\n",
    "oder $L_2$-Regularisierung. Dabei wird die Summe der Quadrate (2-Norm) der\n",
    "Modellkoeffizienten $\\theta_i$ bestraft; in diesem Fall würde die Strafe für die Modellanpassung\n",
    "\n",
    "$P=\\alpha\\sum_{n=1}^{N}\\theta_n^2$\n",
    "\n",
    "wobei $\\alpha$ ein freier Parameter ist, der die Stärke der Strafe steuert. Diese\n",
    "Art von Bestrafungsmodell ist in Scikit-Learn mit dem Ridge-Schätzer eingebaut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "model = make_pipeline(Gauss.GaussianFeatures(30), Ridge(alpha=0.1))\n",
    "basis_plot(model, title='Ridge Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Der Parameter $\\alpha$ ist im Wesentlichen ein Regler, der die Komplexität des\n",
    "resultierenden Modells steuert. Im Limit $\\alpha \\to 0$ erhalten wir das lineare\n",
    "Standard-Regressionsergebnis zurück; im Limit $\\alpha \\to \\infty$ werden alle\n",
    "Modellantworten unterdrückt.\n",
    "\n",
    "Ein Vorteil der Ridge-Regression besteht insbesondere darin, dass sie sehr\n",
    "effizient berechnet werden kann - mit kaum mehr Rechenaufwand als das\n",
    "ursprüngliche lineare Regressionsmodell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lasso-Regression ($L_1$-Regulierung)\n",
    "\n",
    "Eine andere sehr verbreitete Art der Regularisierung ist als \"Lasso\" bekannt und\n",
    "beinhaltet die Bestrafung der Summe der absoluten Werte (1-Normen) der\n",
    "Regressionskoeffizienten:\n",
    "\n",
    "$P=\\alpha\\sum_{n=1}^N|\\theta_n|$\n",
    "\n",
    "Obwohl dies konzeptionell der Ridge-Regression sehr ähnlich ist, können die\n",
    "Ergebnisse überraschend unterschiedlich ausfallen:\n",
    "\n",
    "Beispielsweise neigt die Lasso-Regression aus geometrischen Gründen dazu, wenn\n",
    "möglich \"sparse\" Modelle zu bevorzugen, d.h. sie setzt die Modellkoeffizienten\n",
    "bevorzugt genau auf Null - eine Art der Feature-Selektion.\n",
    "\n",
    "Wir können dieses Verhalten beobachten, wenn wie die Koeffizienten der Lasso-Regression abtragen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = make_pipeline(Gauss.GaussianFeatures(30), Lasso(alpha=0.001))\n",
    "basis_plot(model, title='Lasso Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Bei der Lasso-Regression ist die Mehrzahl der Koeffizienten genau null, wobei das \n",
    "funktionale Verhalten durch eine kleine Untermenge der verfügbaren Basisfunktionen \n",
    "modelliert wird. \n",
    "\n",
    "Wie bei der Ridge-Regularisierung bestimmt der Hyperparameter $\\alpha$ die Stärke der \n",
    "Strafe und sollte z.B. durch Kreuzvalidierung bestimmt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot von $L_1$ und $L_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(15,7))\n",
    "\n",
    "x = np.linspace(-1,1,100)\n",
    "y = np.asarray([np.abs(x), x**2])\n",
    "for i in [0,1]:\n",
    "    axs[i].plot(x, y[i])\n",
    "    axs[i].axvline(0, c='grey')\n",
    "    axs[i].axhline(0, c='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
