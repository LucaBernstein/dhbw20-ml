{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz\n",
    "\n",
    "Wir betrachten wieder einen Datensatz mit handgeschriebenen Ziffern, diesmal allerdings in etwas höherer Auflösung (28x28 statt 8x8). Außerdem haben wir wesentlich mehr Datensätze zur Verfügung.\n",
    "\n",
    "Die 28x28 Pixel ergeben 784 Features, d.h. unser Ausgangsraum ist $\\mathbb{R}^{784}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/mnist/train.csv')\n",
    "target = train.label\n",
    "train = train.drop(\"label\",axis=1)\n",
    "m,n = train.shape\n",
    "print(f'Wir haben {m} Datenpunkte mit {n} Features')\n",
    "print('Drei Beispielbilder:')\n",
    "fig, axs = plt.subplots(1,3, figsize=(21,7))\n",
    "for idx, i in enumerate([1,200,40000]):\n",
    "    axs[idx].imshow(np.asarray(train.iloc[i]).reshape(28,28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skalierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = train\n",
    "scaler = StandardScaler().fit(X)\n",
    "print(f'Skalierer: {scaler}')\n",
    "X_std = scaler.transform(X)\n",
    "X_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenwertzerlegung\n",
    "\n",
    "Die Eigenvektoren und Eigenwerte der Kovarianz-Matrix stellen den \"Kern\" einer PCA dar: \n",
    "Die Eigenvektoren (Hauptkomponenten/Principal Components) bestimmen die Richtungen\n",
    "des neuen Feature-Raums, und die Eigenwerte bestimmen ihre Größe. \n",
    "\n",
    "Mit anderen Worten: Die Eigenwerte erklären die Varianz der Daten entlang der Achsen (Eigenvektoren) der transformierten Merkmale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov = np.dot((X_std-mean_vec).T, X_std-mean_vec)/(m-1)\n",
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "eig_vals.shape, eig_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sortieren die Eigenwerte nach absteigender Größe, denn sie tragen gemäß ihrer Größe ihren Anteil an der Varianz der Daten (also nach unserer Grundannahme dem Informationsgehalt der Daten) bei:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "\n",
    "var_exp = [(i/tot) for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot der erklärten Varianz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True, figsize=(15,7))\n",
    "\n",
    "axs[0].plot(list(range(n)), cum_var_exp, c='black', label='Kumulierte erklärte Varianz')\n",
    "axs[0].plot(list(range(n)), var_exp, c='red', label='Erklärte Varianz')\n",
    "axs[0].set_xlabel('Eigenvektor')\n",
    "axs[0].axhline(1, c='grey')\n",
    "axs[0].legend();\n",
    "\n",
    "k = 250\n",
    "axs[1].plot(list(range(k)), cum_var_exp[:k], c='black', label='Kumulierte erklärte Varianz')\n",
    "axs[1].plot(list(range(k)), var_exp[:k], c='red', label='Erklärte Varianz')\n",
    "axs[1].set_xlabel(f'Die ersten {k} Eigenvektoren')\n",
    "axs[1].axhline(1, c='grey')\n",
    "axs[1].axhline(cum_var_exp[k], c='grey', linestyle='dotted')\n",
    "axs[1].text(0, cum_var_exp[k], f'{cum_var_exp[k]:.5f}')\n",
    "axs[1].legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "D.h. mit den ersten 250 der 784 Principal-Component-Features würden wir schon **über 90% der Varianz** abdecken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn's PCA\n",
    "\n",
    "Scikit spart uns die Mühe selber Kovarianz und Eigenwerte zu ermitteln und verpackt PCA in einen fertigen Algorithmus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X_std)\n",
    "\n",
    "eigenvectors = pca.components_\n",
    "cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,7))\n",
    "\n",
    "ax.plot(list(range(n)), pca.explained_variance_ratio_, c='red', label='Erklärte Varianz')\n",
    "ax.plot(list(range(n)), cumul, c='black', label='Kumulierte erklärte Varianz')\n",
    "ax.set_xlabel('Eigenvektor')\n",
    "ax.axhline(1, c='grey')\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotte Eigenvektoren\n",
    "\n",
    "Unsere Eigenvektoren sind Vektoren in $\\mathbb{R}^{784}$. Gleichzeitig sind diese Vektoren aber ja auch jeweils die Pixel eines 28x28 Bildes.\n",
    "\n",
    "D.h. wir können auch unsere Eigenvektoren als Bilder anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = 7\n",
    "n_col = 7\n",
    "\n",
    "fig, axs = plt.subplots(n_row, n_col, figsize=(20,3*n_row))\n",
    "\n",
    "def plot_eigenvectors(row, col, eigenvector_nr):\n",
    "    ax = axs[row, col]\n",
    "    ax.imshow(eigenvectors[eigenvector_nr].reshape(28,28), cmap='gray') # Alternativ in Farbe: jet\n",
    "    title_text = f'Eigenvektor {eigenvector_nr + 1}'\n",
    "    ax.set_title(title_text, size=12)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "offsets = [0, 10, 50, 100, 200, 500, n-n_col]\n",
    "\n",
    "for i in list(range(n_row)):\n",
    "    for j in range(n_col):\n",
    "        plot_eigenvectors(i, j, offsets[i] + j)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen, dass die Eigenvektoren mit großen Strukturen anfangen und in der Folge immer feiner werden. Das passt zu unserer Intuition, dass mit den größten Eigenwerten zunächst die wichtigsten Features erfasst werden und am Schluss der Reihe nur noch kleinste Details abgebildet werden.\n",
    "\n",
    "In diesem Fall hier kann man tatsächlich den Featureraum um etwa 200 Dimensionen verkleinern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deutung der transformierten Features\n",
    "\n",
    "Ein Nachteil von PCA ist, dass die transformierten Features i.d.R. schwer zu erklären sind.\n",
    "\n",
    "Schauen wir uns drei Bilder im transformierten Raum an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axst, axsr) = plt.subplots(2,3, figsize=(21,14))\n",
    "for idx, i in enumerate([1,200,40000]):\n",
    "    image = np.asarray(train.iloc[i])\n",
    "    transformed = pca.transform([image])\n",
    "    axst[idx].imshow(transformed.reshape(28,28), cmap='gray');\n",
    "    axsr[idx].imshow(image.reshape(28,28), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
