{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz\n",
    "\n",
    "Wir betrachten wieder eine Datensatz mit handgeschriebenen Ziffern, diesmal allerdings in etwas höherer Auflösung (28x28 statt 8x8). Außerdem haben wir wesentlich mehr Datensätze zur Verfügung.\n",
    "\n",
    "Die 28x28 Pixel ergeben 784 Features, d.h. unser Ausgangsraum ist $\\mathbb{R}^{784}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/mnist/train.csv')\n",
    "target = train.label\n",
    "train = train.drop(\"label\",axis=1)\n",
    "m,n = train.shape\n",
    "print(f'Wir haben {m} Datenpunkte mit {n} Features')\n",
    "print('Ein Beispielbild:')\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "ax.imshow(np.asarray(train.iloc[1]).reshape(28,28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufbereitung\n",
    "\n",
    "Wir standardisieren die Features durch Entfernen des Mittelwerts und Skalierung auf Einheitsvarianz.\n",
    "\n",
    "Der Standardwert einer Stichprobe x wird berechnet als:\n",
    "\n",
    "$$z=\\frac{x-\\mu}{\\sigma}$$\n",
    "\n",
    "wobei $\\mu$ der Mittelwert der Trainingsstichproben und $\\sigma$ die Standardabweichung der Trainingsstichproben ist.\n",
    "\n",
    "Zentrierung und Skalierung erfolgen unabhängig für jedes Merkmal, indem die relevanten Statistiken über die Stichproben im Trainingsset berechnet werden. Mittelwert und Standardabweichung werden dann gespeichert, um bei späteren Daten mittels Transformation verwendet zu werden.\n",
    "\n",
    "Die Standardisierung eines Datensatzes ist eine häufige Anforderung für viele Schätzer des maschinellen Lernens: Sie könnten sich schlecht verhalten, wenn die einzelnen Merkmale nicht mehr oder weniger wie normalverteilte Standarddaten aussehen (z.B. Gaußsch mit Mittelwert 0 und Einheitsvarianz).\n",
    "\n",
    "Beispielsweise gehen viele Elemente, die in der Cost-Function eines Lernalgorithmus verwendet werden (wie die L1- und L2-Regularisierer von linearen Modellen) davon aus, dass alle Merkmale um 0 zentriert sind und Varianzen in gleicher Größenordnung haben. Wenn ein Merkmal eine Varianz hat, die um Größenordnungen größer ist als andere, könnte es die Zielfunktion dominieren und den Schätzer unfähig machen, von anderen Merkmalen zu lernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = train\n",
    "scaler = StandardScaler().fit(X)\n",
    "print(f'Skalierer: {scaler}')\n",
    "X_std = scaler.transform(X)\n",
    "X_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenwertzerlegung\n",
    "\n",
    "Die Eigenvektoren und Eigenwerte einer Kovarianz- Matrix stellen den \"Kern\" einer PCA dar: \n",
    "Die Eigenvektoren (Hauptkomponenten/Principal Components) bestimmen die Richtungen\n",
    "des neuen Featureraums, und die Eigenwerte bestimmen ihre Größe. \n",
    "\n",
    "Mit anderen Worten: Die Eigenwerte erklären die Varianz der Daten entlang der Achsen (Eigenvektoren) des neuen Merkmals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berechne die Kovarianz Matrix\n",
    "\n",
    "Der klassische Ansatz der PCA besteht darin, die Eigenwertzerlegung auf der Kovarianzmatrix $Cov(X)$ durchzuführen, die eine n×n-Matrix ist, in der jedes Element die Kovarianz zwischen\n",
    "zwei Merkmalen repräsentiert. \n",
    "\n",
    "Die Kovarianz zwischen zwei Merkmalen wird wie folgt berechnet:\n",
    "\n",
    "$$\\sigma_{jk}=\\frac{1}{n-1}\\sum\\limits_{i=1}^n{(x_i^{(j)}-\\mu_j)(x_i^{(k)}-\\mu_k)}$$\n",
    "\n",
    "In Matrizenform geschrieben ist das:\n",
    "\n",
    "$$Cov(X)=\\frac{1}{n-1}((X-\\mu)^T (X-\\mu))$$\n",
    "\n",
    "mit $\\mu=\\frac{1}{n}\\sum\\limits_{i=1}^n x^{(i)}$.\n",
    "\n",
    "Die Kovarianzmatrix drückt die paarweisen Abhängigkeiten der Features untereinander aus, sie\n",
    "ist also eine Verallgemeinerung der Korrelation in den n-dimensionalen Raum. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov = np.dot((X_std-mean_vec).T, X_std-mean_vec)/(m-1)\n",
    "cov.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenwerte und -vektoren\n",
    "\n",
    "Für einen Eigenvektor $v$ einer Matrix X gilt: $v\\neq 0$ und es existiert ein $\\lambda$, so dass $$Xv=\\lambda v$$.\n",
    "\n",
    "Die Eigenvektoren bilden eine Orthonormalbasis, d.h. wir können jeden Vektor als Linearkombination der Eigenvektoren schreiben.\n",
    "\n",
    "Wenn wir die Eigenwerte unserer Kovarianzmatrix ermitteln, so spiegelt die Größe der Eigenwerte wieder, wie stark der Einfluss dieser Richtung auf unsere Daten ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "eig_vals.shape, eig_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteil an Varianz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "\n",
    "var_exp = [(i/tot) for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot der erklärten Varianz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True, figsize=(15,7))\n",
    "\n",
    "axs[0].plot(list(range(n)), cum_var_exp, c='black', label='Kumulierte erklärte Varianz')\n",
    "axs[0].plot(list(range(n)), var_exp, c='red', label='Erklärte Varianz')\n",
    "axs[0].set_xlabel('Eigenvektor')\n",
    "axs[0].axhline(1, c='grey')\n",
    "axs[0].legend();\n",
    "\n",
    "k = 250\n",
    "axs[1].plot(list(range(k)), cum_var_exp[:k], c='black', label='Kumulierte erklärte Varianz')\n",
    "axs[1].plot(list(range(k)), var_exp[:k], c='red', label='Erklärte Varianz')\n",
    "axs[1].set_xlabel(f'Die ersten {k} Eigenvektoren')\n",
    "axs[1].axhline(1, c='grey')\n",
    "axs[1].axhline(cum_var_exp[k], c='grey', linestyle='dotted')\n",
    "axs[1].text(0, cum_var_exp[k], f'{cum_var_exp[k]:.5f}')\n",
    "axs[1].legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "D.h. mit den ersten 250 der 784 Principal-Component-Features würden wir schon **über 90% der Varianz** abdecken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn's PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X_std)\n",
    "\n",
    "eigenvectors = pca.components_\n",
    "cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,7))\n",
    "\n",
    "ax.plot(list(range(n)), pca.explained_variance_ratio_, c='red', label='Erklärte Varianz')\n",
    "ax.plot(list(range(n)), cumul, c='black', label='Kumulierte erklärte Varianz')\n",
    "ax.set_xlabel('Eigenvektor')\n",
    "ax.axhline(1, c='grey')\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotte Eigenvektoren\n",
    "\n",
    "Unsere Eigenvektoren sind Vektoren in $\\mathbb{R}^{784}$. Gleichzeitig sind diese Vektoren aber ja auch jeweils die Pixel eines 28x28 Bildes.\n",
    "\n",
    "D.h. wir können auch unsere Eigenvektoren als Bilder anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = 7\n",
    "n_col = 7\n",
    "\n",
    "fig, axs = plt.subplots(n_row, n_col, figsize=(20,3*n_row))\n",
    "\n",
    "def plot_eigenvectors(row, col, eigenvector_nr):\n",
    "    ax = axs[row, col]\n",
    "    ax.imshow(eigenvectors[eigenvector_nr].reshape(28,28), cmap='gray') # Alternativ in Farbe: jet\n",
    "    title_text = f'Eigenvektor {eigenvector_nr + 1}'\n",
    "    ax.set_title(title_text, size=12)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "offsets = [0, 10, 50, 100, 200, 500, n-n_col]\n",
    "\n",
    "for i in list(range(n_row)):\n",
    "    for j in range(n_col):\n",
    "        plot_eigenvectors(i, j, offsets[i] + j)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('dhbw20': conda)",
   "language": "python",
   "name": "python37664bitdhbw20conda4e9f90a409f24fc4a1b2fdf8774acce3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
