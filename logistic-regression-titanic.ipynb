{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistische Regression auf dem Titanic Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pltdefaults as plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hole Daten\n",
    "\n",
    "Der Datensatz stammt von [Kaggle](https://www.kaggle.com/sureshbhusare/titanic-dataset-from-kaggle).\n",
    "\n",
    "Er beinhaltet Daten zu den über 1.300 Passagieren der tragischen Jungfernfahrt. Die Daten der über 900 Mann starken Besatzung sind nicht enthalten.\n",
    "\n",
    "Die Spalten haben folgende Bedeutung:\n",
    "\n",
    "Variable\t|Definition\t|Key\n",
    ":---|:---|---\n",
    "Survival\t|Survival - Label\t|0 = No, 1 = Yes\n",
    "Pclass\t|Ticket class\t|1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "Sex\t|Sex\t| male, female\n",
    "Age\t|Age in years|\t\n",
    "SibSp\t|# of siblings / spouses aboard the Titanic\t|\n",
    "Parch\t| # of parents / children aboard the Titanic\t|\n",
    "Ticket\t|Ticket number\t|\n",
    "Fare\t|Passenger fare\t|\n",
    "Cabin\t|Cabin number\t|\n",
    "Embarked\t|Port of Embarkation\t|C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "Name|Name des Passagiers |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/titanic/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein erster Blick auf die Daten\n",
    "\n",
    "`shape` sagt uns die Dimensionen des Datensatzes: Wieviele Datensätze (Zeilen) und wieviele Features (Spalten) haben wir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `head` oder `tail` können wir uns die ersten bzw. letzten paar Datensätze anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`info` gibt uns ein paar Informationen zu den Datentypen der Spalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`describe` liefert uns einige Kennzahlen zur statistischen Verteilung der Daten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe(exclude='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir konnten aus `train.info()` schon ablesen, dass zu \"Age\", \"Embarked\" und \"Cabin\" weniger non-null Werte als 891 angezeigt werden. Zu diesen Null-Werten müssen wir noch überlegen, wie wir damit umgehen.\n",
    "\n",
    "Hier können wir uns ein Bild davon machen, wo die Null-Werte auftreten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(data=train.isnull(), ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "... wie sehen unsere Daten eigentlich aus?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotten wir mal eine Verteilung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f, axs = plt.subplots(2, 4, figsize=(20, 7), sharex=False)\n",
    "\n",
    "showColumns = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "\n",
    "r = 0\n",
    "c = 0\n",
    "for col in showColumns:\n",
    "    if col == 'Age' or col == 'Fare':\n",
    "        sns.distplot(train[col], ax=axs[r, c])\n",
    "    else:\n",
    "        sns.countplot(train[col], ax=axs[r, c])\n",
    "        \n",
    "    if col == 'Fare':\n",
    "        axs[r, c].xaxis.set_major_locator(ticker.MultipleLocator(100))\n",
    "        axs[r, c].xaxis.set_minor_locator(ticker.MultipleLocator(10))\n",
    "        axs[r, c].xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    if col == 'Age':\n",
    "        axs[r, c].xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "        axs[r, c].xaxis.set_minor_locator(ticker.MultipleLocator(5))\n",
    "        axs[r, c].xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    c = c + 1\n",
    "    if (c > 3):\n",
    "        r = 1\n",
    "        c = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "In der Statistik misst der Korrelationskoeffizient $\\rho$ die Stärke und Richtung einer linearen Beziehung zwischen zwei Variablen.\n",
    "\n",
    "Ab einem Absolutwert größer 0.5 sollte man die Korrelation betrachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(train.corr(), annot=True, center=0, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Korrelation zwischen Fare und Pclass ist jetzt nicht überraschend. aber ggf. könnte es Sinn machen Fare zu droppen um Kollinearität zu vermeiden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Aufbereitung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fehlende Werte\n",
    "\n",
    "Die Spalte Cabin droppen wir, da uns zu viele Werte fehlen.\n",
    "\n",
    "Die zwei fehlenden Werte in der Spalte Embarked können wir einfach mit dem häufigsten Hafen belegen: 'S'.\n",
    "\n",
    "Die Spalte \"Age\" hat einige fehlende Einträge. Wir haben die Alternativen entweder die Spalte auch zu droppen, \n",
    "dann verlieren wir allerdings einige Information, oder die fehlenden Werte aufzufüllen.\n",
    "\n",
    "Ein Ansatz dazu, ist zu schauen, wo die höchste Korrelation zu \"Age\" besteht und dies zum Auffüllen zu verwenden:\n",
    "\"Pclass\" ist am höchsten korreliert mit \"Age\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_group = train.groupby(\"Pclass\")[\"Age\"]\n",
    "print(age_group.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline\n",
    "\n",
    "Wir packen die ganzen Umformungen in eine Funktion, da wir sie auch für den Test-Datensatz brauchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(data, scaler=None):\n",
    "    X = data.copy()\n",
    "    y = X['Survived']\n",
    "    \n",
    "    # Spalten droppen, die wir nicht weiter betrachten\n",
    "    # PassengerId, Name und Ticket scheinen auf den ersten Blick wenig relevant zu sein, \n",
    "    # also droppen wir sie für unseren ersten Ansatz.\n",
    "    # Cabin ist nur zu einem geringen Teil gefüllt, und eine Methode zur Befüllung der \n",
    "    # leeren Daten liegt nicht auf der Hand. Also droppen wir Cabin auch:\n",
    "    X.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived'], axis=1, inplace=True)\n",
    "     \n",
    "    # Der Altersmedian unterscheided sich zwischen den Klassen signifikant, also setzen wir \n",
    "    # diesen für die fehlenden Werte ein\n",
    "    X.loc[X.Age.isnull(), 'Age'] = X.groupby('Pclass').Age.transform('median')\n",
    "    \n",
    "    # Für die zwei fehlenden Ausgangshäfen nehmen wir einfach den häufigsten Wert an - \"S\"\n",
    "    X.Embarked = X.Embarked.fillna('S')\n",
    "    \n",
    "    # Skalierung der Daten\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler().fit(X[['Age', 'Fare']])\n",
    "    X[['Age', 'Fare']] = scaler.transform(X[['Age', 'Fare']])\n",
    "    \n",
    "    ### One-hot Encoding \"Sex\" und \"Embarked\"\n",
    "    # Pandas hat eine sehr nützliche Funktion, die kategorische Variablen One-Hot encoded und \n",
    "    # den Dataframe gleich entsprechend umwandelt - diese heißt `get_dummies`.\n",
    "    # Der Name rührt daher, dass mit dem On-Hot Encoding neue Spalten (= neue Variablen) \n",
    "    # entstehen, die \"Dummy-Variablen\" genannt werden - da sie in gewissem Sinne ja keine \n",
    "    # \"echten\" Variablen sind.\n",
    "    # Bei Scikit-Learn leistet dasselbe der `OneHotEncoder`.\n",
    "    return scaler, pd.get_dummies(X, drop_first=True), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbereitung der Daten über die Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler, X, y = data_pipeline(train)\n",
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie sehen unsere aufbereiteten Daten jetzt aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsere Altersverteilung sieht jetzt so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 8))\n",
    "\n",
    "rescaled = scaler.inverse_transform(X[['Age', 'Fare']])\n",
    "sns.distplot(rescaled[:,0])\n",
    "plt.title(\"Age Histogram\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vorbereiten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir splitten unsere Daten noch in ein Training- und ein Test-Set im Verhältnis 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistische Regression\n",
    "\n",
    "Trainiere das Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = model.fit(X_train, y_train)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... und bewerte das Ergebnis ...\n",
    "\n",
    "**Confusion Matrix:** Die Confusion-Matrix $C$ ist definiert durch: $C_{i,j}$ ist die Anzahl der Beobachtungen der wahren Gruppe $i$, die als zur Gruppe $j$ vorhergesagt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(\"\\t'false'\\t'true'\")\n",
    "print(\"false\\t  {}\\t  {}\".format(tn, fp))\n",
    "print(\"true\\t  {}\\t  {}\".format(fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bewertung des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kennzahlen zur Bewertung\n",
    "\n",
    "**Precision** ist das Verhältnis $\\frac{tp}{tp + fp}$, wobei $tp$ die Anzahl der echten positiven und $fp$ die Anzahl der falschen positiven Werte ist. Intuitiv ist Precision die Fähigkeit des Klassifikators, eine negative Probe nicht als positiv zu kennzeichnen.\n",
    "\n",
    "**Recall** ist das Verhältnis $\\frac{tp}{tp + fn}$, wobei $fn$ die Anzahl der falschen Negativen ist. Intuitiv ist Recall die Fähigkeit des Klassifikators, alle positiven Proben zu finden.\n",
    "\n",
    "Der **F-1-Score** kann als ein gewichteter harmonischer Mittelwert von Precision und Recall interpretiert werden, wobei ein F-Beta-Score seinen besten Wert bei 1 und den schlechtesten Wert bei 0 erreicht.\n",
    "\n",
    "$f1=2\\times\\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\n",
    "**Support** ist die jeweilige Anzahl der Vorkommnisse der wahren Labels.\n",
    "\n",
    "**weighted avg** gewichtet den Durchschnitt mit der Häufigkeit der Klassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions, target_names=['Nicht überlebt', 'Überlebt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation des Modells\n",
    "\n",
    "Schauen wir uns Intercept und Koeffizienten an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept/Bias: {}'.format(model.intercept_))\n",
    "\n",
    "coef_dict = sorted(list(zip(X.columns.tolist(), model.coef_.ravel())), key=lambda tup: -abs(tup[1]))\n",
    "for tup in coef_dict:\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Geschlecht ist mit Abstand der am stärksten eingehende Faktor, gefolgt von der Klasse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übung\n",
    "\n",
    "Versuchen Sie das Modell zu verbessern, indem Sie z.B. die Familiengröße verwenden oder aus dem Namen den Titel extrahieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set([t.split(',')[1].split('.')[0] for t in train.Name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
