{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf_config = tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "tf_config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation der MNIST Ziffern\n",
    "\n",
    "Wir laden uns die 60.000 Trainings- und 10.000 Test-Bilder im Format 28x28.\n",
    "\n",
    "**Beachte:** Die Daten werden einmalig heruntergeladen und lokal persistiert - das erste Mal kann abhängig von Bandbreite etwas länger dauern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Construct a tf.data.Dataset\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten vorbereiten\n",
    "\n",
    "Erzeuge aus unserem Dataset einen Generator für die Samples.\n",
    "\n",
    "Tensorflow Datasets haben den großen Vorteil, dass sie nicht in den Speicher passen müssen, da\n",
    "die Daten als Stream zur Verfügung gestellt werden.\n",
    "\n",
    "Die Dataset-Verwendung folgt einem gemeinsamen Muster:\n",
    "\n",
    "-  Erstellen Sie einen Quelldatensatz aus Ihren Eingabedaten.\n",
    "-  Wenden Sie Dataset-Transformationen zur Vorverarbeitung der Daten an.\n",
    "-  Iterieren Sie über das Dataset und verarbeiten Sie die Elemente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Grauwerte müssen wir zunächst von [0 ... 255] zu [0.0 ... 1.0] konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train_scaled = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test_scaled = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes erzeugen wir unseren Datengenerator, über den das Training nachher iteriert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds_train_scaled.shuffle(1024, reshuffle_each_iteration=True).batch(64).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ebenso erzeugen wir unseren Test-Generator, allerdings hier ohne Shufflen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds_test_scaled.batch(64).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition des neuronalen Netzes\n",
    "\n",
    "Unser Modell konstruieren wir aus einer Sequenz von Layern.\n",
    "\n",
    "Im **Inputlayer** wandeln wir die 28x28 Pixel in einen 784-Vektor um.\n",
    "\n",
    "Der **erste hidden Layer** ist ein voll verbundener Layer mit 128 Nodes und ReLU-Aktivierung. \"input_shape\" muss nur beim Input-Layer angegeben werden, danach weiß Tensorflow ja selbst, was die Ausgabe des vorigen und damit die Eingabe des Folgelayers ist.\n",
    "\n",
    "Der **zweite hidden Layer** ist ein **Dropout**-Layer. Der Parameter 0.2 besagt, dass das Netz in jeder Iteration zufällig 20% der Weights vergisst. Dies ist eine Form der Regularisierung (reduziert das Auswendiglernen von Trainingsdaten).\n",
    "\n",
    "Der **Output-Layer** schließlich ist ein voll verbundener Layer mit 10 Nodes für unsere 10 Klassen. Eine Aktivierungsfunktion ist an diesem Layer nicht angegeben, daher wird die Default-Aktivierung verwendet, das ist schlicht die Identity-Funktion $f(x)=x$.\n",
    "\n",
    "D.h. unser Outputlayer bildet einfach eine gewichtete Summen über die 128 Nodes des vorherigen Layers.\n",
    "\n",
    "Mit den Bezeichnungen unserer Folien ergibt sich daher (wenn wir die Ausgabe des Flatten-Layer als Inputlayer betrachten):\n",
    "\n",
    "$$a^{[1]}=ReLU({w^{[1]}}^Tx+b^{[1]})$$\n",
    "$$a^{[2]}=Dropout(a^{[1]})$$\n",
    "$$a^{[3]}={w^{[3]}}^Ta^{[2]}+b^{[3]}=\\hat{y}$$\n",
    "\n",
    "mit $x \\in \\mathbb{R}^{128}$ und $\\hat{y} \\in \\mathbb{R}^{10}$.\n",
    "\n",
    "**Dropout** führt dazu, dass 20% der $a^{[1]}$ auf 0 gesetzt und die anderen proportional erhöht werden, so dass $\\sum a^{[1]}=\\sum a^{[2]}$ gilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "print(model.summary())\n",
    "print(f'Anzahl Parameters Layer Dense 128: #a * #w + #b = 784 * 128 + 128 = {784 * 128 + 128}')\n",
    "print(f'Anzahl Parameters Layer Dense 10:  #a * #w + #b = 128 * 10 + 10 = {128 * 10 + 10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ds_train_scaled.as_numpy_iterator()\n",
    "[sample, label] = next(samples)\n",
    "sample.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für jedes Sample gibt das Modell einen Vektor von \"Logits\" oder \"Log-Odds\" zurück, einen für jede Klasse.\n",
    "\n",
    "Tensorflow rechnet mit sogenannten **\"Tensoren\"**. Dies sind schlicht Datenstrukturen, die sowohl Skalare, als auch Vektoren und multidimensionale Matrizen abbilden können. Die Dimension eines Tensors heißt \"Rank\". D.h. ein Skalar ist ein Tensor von Rank=0, ein Vektor hat Rank=1, u.s.w.\n",
    "\n",
    "Wenn wir uns den Wert eines Tensors anschauen wollen, müssen wir die Methode `.numpy()` auf dem Tensor aufrufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_prediction = model(tf.convert_to_tensor([sample]).numpy())\n",
    "print(f'Logit-Werte des untrainierten Modells für das erste Bild: {untrained_prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die **Softmax Funktion** für einen n-dimensionalen Vektor $z$ ist definiert als:\n",
    "$$\\sigma(z)_j=\\frac{e^{z_i}}{\\sum\\limits_{i=0}^n e^{z_i}}$$\n",
    "\n",
    "Die Werte der $\\sigma(z)_j$ für jedes $j$ liegen dabei zwischen 0 und 1, und die Summe der $\\sigma(z)_j$ addiert sich zu 1.\n",
    "\n",
    "Der Wert kann daher als Wahrscheinlichkeit für die jeweilige Klasse $j$ interpretiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Untrainierte Wahrscheinlichkeit für die Klassen bzgl des ersten Bildes: {tf.nn.softmax(untrained_prediction).numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der **SparseCategoricalCrossentropy**-Verlust nimmt einen Vektor von Logits und einen True-Index und gibt für jedes Sample den skalaren Verlust aus.\n",
    "\n",
    "Dieser Verlust ist gleich der negativen Log-Wahrscheinlichkeit der wahren Klasse: Sie ist Null, wenn das Modell sicher ist, dass es die richtige Klasse hat.\n",
    "\n",
    "Das verallgemeinert die Loss-Funktion, die wir aus der logistischen Regression kennen\n",
    "$$-y log(\\hat{y})-(1-y) log(1-\\hat{y})$$\n",
    "auf den Multi-Label Fall.\n",
    "\n",
    "**Beachte:** SparseCategorical... ist für den Fall, wo die Multi-Label direkt über ihren Index angesprochen werden, CategoricalCrossentropy leistet dasselbe für One-hot encoded Labels, also statt `5` im ersten Fall, `[0, 0, 0, 0, 0, 1, 0,..., 0]` im letzteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser **untrainiertes Modell** gibt Wahrscheinlichkeiten nahe dem Zufallsprinzip an (1/10 für jede Klasse), so dass der Anfangsverlust nahe bei $-log(1/10) \\approx 2.3$ liegen sollte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Für den True-Index {label} erhalten wir den Loss {loss_fn(label, untrained_prediction).numpy()} für das untrainierte Modell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation des Modells\n",
    "\n",
    "Der `compile`-Step konfiguriert das Modell für das Training: in diesem Schritt werden die Loss-Funktion, der Optimizer (Verfeinierung des Gradient Descent-Verfahrens) sowie eine auszuweisende Metrik festgelegt.\n",
    "\n",
    "*Beachte:* 'accuracy' im Zusammenspiel mit dem Loss `SparseCategoricalCrossentropy` ist `tf.keras.metrics.SparseCategoricalAccuracy` nicht `tf.metrics.Accuracy`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training des Modells\n",
    "\n",
    "Die Methode `Model.fit` passt die Modellparameter an, um den Verlust zu minimieren.\n",
    "\n",
    "Es werden je Epoche Zwischenergebnisse ausgegeben. Eine **Epoche** ist ein Durchlauf durch die Trainingsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, epochs=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unter den Epochen zeigt Tensorflow die Anzahl der Iterationsschritte pro Epoche an. Da wir eine\n",
    "Batch-Size von 64 festgelegt hatten, bekommen wir 938 Schritte pro Epoche: wir benötigen 938\n",
    "Minibatch Schritte bei einer Batchsize von 64, um die 60.000 Fälle abzudecken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des Modells\n",
    "\n",
    "Mit `Model.evaluate` überprüfen wir jetzt die Performanz des Modells mit unserem Test-Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, accuracy) = model.evaluate(test, verbose=1)\n",
    "print(f'\\nLoss: {loss:.4f}, Accuracy: {accuracy*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Wir könnten den Softmax-Layer auch direkt in unser Modell integrieren, um als Ausgabe direkt eine Wahrscheinlichkeit für die jeweilige Klasse zu erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ds_test_scaled.batch(1)\n",
    "for idx, sample in enumerate(samples):\n",
    "    print(f'Sample Nr. {idx}\\n')\n",
    "    print(f'Logit-Ausgabe: {model(sample)}')\n",
    "    print(f'Softmax-Ausgabe: {probability_model(sample)*100}\\n')\n",
    "    if idx > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('dhbw20': conda)",
   "language": "python",
   "name": "python37664bitdhbw20conda4e9f90a409f24fc4a1b2fdf8774acce3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
