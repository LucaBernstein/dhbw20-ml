{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regression\n",
    "\n",
    "Lineare Regressionsmodelle sind ein guter Ausgangspunkt für Regressionsaufgaben.\n",
    "\n",
    "Solche Modelle sind beliebt, weil sie sehr schnell angepasst werden können und sehr gut interpretierbar sind. \n",
    "\n",
    "Die einfachste Form eines linearen Regressionsmodells ist die Anpassung einer geraden Linie an Daten, wir werden nachher aber auch sehen, wie man kompliziertere Modelle anpassen kann.\n",
    "\n",
    "Wir beginnen mit den Standard-Importen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einfache lineare Regression\n",
    "\n",
    "Eine Gerade ist durch die Gleichung\n",
    "\n",
    "$y = b + wx$\n",
    "\n",
    "gegeben, wobei $w$ die Steigung der Gerade ist und $b$ als y-Achsenabschnitt/Bias/Intercept bezeichnet wird.\n",
    "\n",
    "Generieren wir uns ein paar Daten, die um die Gerade $y = -5 + 2x$ gestreut sind:\n",
    "\n",
    "> NumPys `np.ramdom.default_rng` gibt einen Pseudo-Zufallsgenerator zurück. Damit wir bei unseren Experimenten\n",
    "> nachvollziehbare Werte bekommen, initiieren wir ihn mit einem festen `seed` - hier 1.\n",
    ">\n",
    "> `random` gibt Werte im Bereich $[0, 1)$ zurück. Mit `random(100)` erhalten wir ein Array von 100 Zufallswerten.\n",
    ">\n",
    "> `standard_normal` gibt analog Werte zurück, allerdings diesmal Standard-Normalverteilt. `random` ist gleichverteilt.\n",
    "\n",
    "**Beachte:** Obwohl `x` ein Array ist, können wir damit in normaler \"skalarer\" Schreibweise `y` als Array errechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m = 100\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "x = 10 * rng.random(m)\n",
    "y = 2 * x - 5 + rng.standard_normal(m)\n",
    "sns.scatterplot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können Scikit-Learns `LinearRegression Estimator` verwenden, um die beste Gerade zu ermitteln:\n",
    "\n",
    "> `np.linspace` erzeugt ein \"linear verteiltes\" Array. D.h. `np.linspace(0, 10, 1000)` verteilt ermittelt 1000 gleichweit verteilte Punkte zwischen 0 und 10.\n",
    "\n",
    "> `np.newaxis` erhöht die Dimension eines Vektors oder einer Matrix: aus einer ein-dimensionalen Struktur wird eine zwei-Dimensionale etc.\n",
    "\n",
    "\n",
    "> `x[:,np.newaxis]` erzeugt aus dem Vektor (1D) $[x^{(1)}, x^{(2)}, \\dots, x^{(n)}]$ eine Matrix (2D) $[[x^{(1)}], [x^{(2)}], \\dots, [x^{(n)}]]$ - unsere Design-Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 10 * rng.random(5)\n",
    "\n",
    "print('Ursprüngliches Array v: {} - {}'.format(v.shape, v))\n",
    "print('Nach v[:, np.newaxis]: {} - \\n{}'.format(v[:, np.newaxis].shape, v[:, np.newaxis]))\n",
    "print('Nach v[np.newaxis, :]: {} - {}'.format(v[np.newaxis, :].shape, v[np.newaxis, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`variable.shape()` ist dein Freund: Es hilft regelmäßig zu prüfen, ob die Dimensionen der Matrizen tatsächlich zusammen passen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "sns.scatterplot(x, y)\n",
    "sns.lineplot(xfit, yfit);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Steigung und der y-Achsenabschnitt sind in den Anpassungsparametern des\n",
    "Modells enthalten. Diese sind in Scikit-Learn immer durch einen abschließenden Unterstrich\n",
    "gekennzeichnet. Hier sind es die Parameter `coef_` und `intercept_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Modell: y = {} + {} * x\".format(model.intercept_, model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wir sehen, dass die Ergebnisse sehr nahe an den Inputs liegen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariater Fall\n",
    "\n",
    "Der LinearRegression-Schätzer ist jedoch noch viel leistungsfähiger.\n",
    "Er kann neben einfachen Geradengleichungen auch mehrdimensionale lineare Modelle\n",
    "der Form\n",
    "\n",
    "$y=a_0+a_1x_1+a_2x_2+\\dots$\n",
    "\n",
    "mit höher-dimensionalen x ermitteln.\n",
    "\n",
    "Geometrisch entspricht dies der Anpassung einer Ebene an Punkte in drei Dimensionen\n",
    "oder der Anpassung einer Hyperebene an Punkte in höheren Dimensionen.\n",
    "\n",
    "Die mehrdimensionale Natur solcher Regressionen macht es schwieriger, sie zu\n",
    "visualisieren, aber wir können ja einfach mal testen, ob die lineare Regression die Parameter für Eingabedaten\n",
    "**ohne Fehler** ermittelt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "X = rng.random((4, 3))\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "\n",
    "model.fit(X, y)\n",
    "print(f'X: {X}')\n",
    "print(f'X.shape: {X.shape}')\n",
    "print(f'y: {y}')\n",
    "print(f'Bias: {model.intercept_}')\n",
    "print(f'Weights: {model.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wir ermitteln $y$ als Punkte der Hyperebene, die durch die vier zufälligen Stützpunkte $x$ definiert sind, und die lineare\n",
    "Regression gewinnt die Koeffizienten zurück, die zur Konstruktion der Hyperebene verwendet wurden.\n",
    "\n",
    "> **Beachte:** Oben hatten wir Skalare mit einem Array multipliziert - dies können wir in Python mit dem gewöhnlichen Multiplikations-Operator tun. Hier multiplizieren wir zwei Vektoren: $(3,1)$ mal $(1,3)$ - dafür brauchen wir die [Matrizenmultiplikation (dot-Produkt)](https://de.wikipedia.org/wiki/Matrizenmultiplikation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
